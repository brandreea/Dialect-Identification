# -*- coding: utf-8 -*-
"""Tfidf+SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ueIUUzkg79lb9myJz_FKYZZEUHfSZLey
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from sklearn import svm
from sklearn import preprocessing
from sklearn.metrics import accuracy_score
import math

#Aceste randuri sunt pentru montarea drive-ului. In cazul in care rulati local, le puteti comenta.
from google.colab import drive
drive.mount('/content/drive')

from sklearn.metrics import f1_score
from sklearn.svm import SVC

#citirea etichetelor
 #a se seta calea
 train_labels = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ml-2020-unibuc-3/train_labels.txt', sep='\t',header=None)
 test_labels = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ml-2020-unibuc-3/validation_labels.txt', sep='\t',header=None)
print(train_labels[1])

#a se seta calea
#citirea datelor de antrenare
with open('/content/drive/My Drive/Colab Notebooks/ml-2020-unibuc-3/train_samples.txt', encoding='utf-8') as q:
    train_data = q.readlines()
print(train_data[0])

#separarea id-urilor de texte
train_ids=[]
train_texts=[]
for data in train_data:
  id,text=data.split('\t')
  train_ids.append(id)
  train_texts.append(text)
print(train_texts[0])

#a se seta calea din drive
#citirea datelor de validare
with open('/content/drive/My Drive/Colab Notebooks/ml-2020-unibuc-3/validation_samples.txt', encoding='utf-8') as q:
    test_data = q.readlines()
print(test_data[0])

#separarea id-ului de text
test_ids=[]
test_texts=[]

for data in test_data:
  id,text=data.split('\t')
  test_ids.append(id)
  test_texts.append(text)
print(test_texts[0])

#crearea listelor de etichete
train_label=[c for c in train_labels[1].values]
test_label=np.array([c for c in test_labels[1].values])

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
svc_pipe = Pipeline([
 ('tvec', TfidfVectorizer(norm='l2', token_pattern='\\S*', analyzer='word',lowercase=False)),
 ('svc', SVC())
])
svc_params = {
 'svc__C':[1,10, 100],
 'svc__gamma': [1,0.1,0.001],
 'svc__kernel': [ 'linear','poly', 'rbf']
 
}

svc_pipe.fit(train_texts, train_label)
svc_gs = GridSearchCV(svc_pipe, param_grid=svc_params, cv = 5, verbose =1, n_jobs = -1)
svc_gs.fit(train_texts, train_label)
print(svc_gs.score(train_texts, train_label))

pred=svc_gs.predict(test_texts[0:2656])

print(svc_gs.best_params_)

print(f1_score(test_label, pred,average='macro'),accuracy_score(test_label,pred))

#citirea datelor de test
#a se seta calea

with open('/content/drive/My Drive/Colab Notebooks/ml-2020-unibuc-3/test_samples.txt', encoding='utf-8') as q:
    final_data = q.readlines()
print(final_data[0])
final_ids=[]
final_texts=[]
for data in final_data:
  id,text=data.split('\t')
  final_ids.append(id)
  final_texts.append(text)
print(final_texts[0])

final_pred=svc_gs.predict(final_texts)

import csv
with open('/content/drive/My Drive/Colab Notebooks/innovatorsnn-cript-svm.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["id", "label"])
    count=0
    for i in range(len(final_pred)):
         writer.writerow([final_ids[i], final_pred[i]])
    print(count)